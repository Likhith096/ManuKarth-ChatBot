{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dd420a323447709f6507705594a684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1324b065806441f595f1396938ab6da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tiiuae/falcon-7b-instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        #trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_trainable_parameters(model):\n",
    "#     \"\"\"\n",
    "#     Prints the number of trainable parameters in the model.\n",
    "#     \"\"\"\n",
    "#     trainable_params = 0\n",
    "#     all_param = 0\n",
    "#     for _, param in model.named_parameters():\n",
    "#         all_param += param.numel()\n",
    "#         if param.requires_grad:\n",
    "#             trainable_params += param.numel()\n",
    "#     print(\n",
    "#         f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,874,368 || all params: 6,940,595,072 || trainable%: 0.2719416390698783\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"query_key_value\"\n",
    "        # \"dense\",\n",
    "        # \"dense_h_to_4h\",\n",
    "        # \"dense_4h_to_h\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: What does a cluster definition file contain??\n",
      "<assistant>:\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "<human>: What does a cluster definition file contain??\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./InstructWithOurOwnData\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim='paged_adamw_32bit',\n",
    "    save_steps=250,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=320,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 311\n",
      "})\n",
      "{'answer': 'Management switches on cluster systems are paired into switch stacks, with two switches per stack. The top switch is typically the master switch, while the bottom switch is typically the slave switch.', 'question': 'How are management switches typically paired in cluster systems, and what roles do the top and bottom switches in a stack usually play?'}\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "import json\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Load the JSON file\n",
    "with open(r\"C:\\Users\\BMSCE CSE\\Downloads\\HPEquestionsOurOwn.json\", 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Extract the \"questions\" data\n",
    "questions_data = data[\"text\"]\n",
    "\n",
    "# Create a new list to store the reformatted data\n",
    "formatted_data = []\n",
    "\n",
    "# Iterate through each item in the original data and reformat it\n",
    "for item in questions_data:\n",
    "    # Check if both question and answer are strings before concatenating\n",
    "    if isinstance(item[\"question\"], str) and isinstance(item[\"answer\"], str):\n",
    "        formatted_item = {\n",
    "            \"text\": \"### Human: \" + item[\"question\"] + \" \" + \"###Assistant: \" +item[\"answer\"]\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "    else:\n",
    "        print(\"Skipping item with non-string question or answer:\", item)\n",
    "\n",
    "\n",
    "formatted_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Print the Dataset summary\n",
    "dataset = formatted_dataset\n",
    "print(dataset)\n",
    "print(dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8780937e74b4d6d98010e2edcbf44cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74490aad3f624223840d29e5bbf8a444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_arguments,\n",
    "#     # packing=True,\n",
    "#     packing=False,\n",
    "# )\n",
    "# Assuming 'dataset' is your dataset\n",
    "dataset = dataset.map(lambda x: {'text': str(x['text'])})\n",
    "\n",
    "# Now, you can initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maditi-cs21\u001b[0m (\u001b[33mbmsce\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\wandb\\run-20240507_155401-m3opl2yy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bmsce/huggingface/runs/m3opl2yy' target=\"_blank\">morning-bee-18</a></strong> to <a href='https://wandb.ai/bmsce/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bmsce/huggingface' target=\"_blank\">https://wandb.ai/bmsce/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bmsce/huggingface/runs/m3opl2yy' target=\"_blank\">https://wandb.ai/bmsce/huggingface/runs/m3opl2yy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c0a94a7c204af8899b952d18ee3ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\models\\falcon\\modeling_falcon.py:446: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8029, 'grad_norm': 0.45438313484191895, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 2.5882, 'grad_norm': 0.6797813773155212, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 2.3855, 'grad_norm': 0.6741534471511841, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 2.1299, 'grad_norm': 1.0471246242523193, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 2.1667, 'grad_norm': 0.9779266119003296, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 2.1876, 'grad_norm': 0.8094273209571838, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 2.0298, 'grad_norm': 0.9309846758842468, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 2.0028, 'grad_norm': 0.7519174814224243, 'learning_rate': 0.0002, 'epoch': 1.03}\n",
      "{'loss': 1.9234, 'grad_norm': 0.8262293934822083, 'learning_rate': 0.0002, 'epoch': 1.16}\n",
      "{'loss': 1.9232, 'grad_norm': 0.9563876986503601, 'learning_rate': 0.0002, 'epoch': 1.29}\n",
      "{'loss': 1.8096, 'grad_norm': 1.4543246030807495, 'learning_rate': 0.0002, 'epoch': 1.41}\n",
      "{'loss': 1.8638, 'grad_norm': 1.2386586666107178, 'learning_rate': 0.0002, 'epoch': 1.54}\n",
      "{'loss': 1.9187, 'grad_norm': 0.9437715411186218, 'learning_rate': 0.0002, 'epoch': 1.67}\n",
      "{'loss': 1.8606, 'grad_norm': 1.1027714014053345, 'learning_rate': 0.0002, 'epoch': 1.8}\n",
      "{'loss': 1.8526, 'grad_norm': 0.9789018034934998, 'learning_rate': 0.0002, 'epoch': 1.93}\n",
      "{'loss': 1.7682, 'grad_norm': 1.062335729598999, 'learning_rate': 0.0002, 'epoch': 2.06}\n",
      "{'loss': 1.7155, 'grad_norm': 1.6316981315612793, 'learning_rate': 0.0002, 'epoch': 2.19}\n",
      "{'loss': 1.6165, 'grad_norm': 1.311288833618164, 'learning_rate': 0.0002, 'epoch': 2.32}\n",
      "{'loss': 1.6852, 'grad_norm': 1.1508594751358032, 'learning_rate': 0.0002, 'epoch': 2.44}\n",
      "{'loss': 1.6655, 'grad_norm': 1.8997962474822998, 'learning_rate': 0.0002, 'epoch': 2.57}\n",
      "{'loss': 1.6395, 'grad_norm': 1.394413948059082, 'learning_rate': 0.0002, 'epoch': 2.7}\n",
      "{'loss': 1.7088, 'grad_norm': 1.0934717655181885, 'learning_rate': 0.0002, 'epoch': 2.83}\n",
      "{'loss': 1.6703, 'grad_norm': 1.1331919431686401, 'learning_rate': 0.0002, 'epoch': 2.96}\n",
      "{'loss': 1.521, 'grad_norm': 1.330031394958496, 'learning_rate': 0.0002, 'epoch': 3.09}\n",
      "{'loss': 1.3428, 'grad_norm': 1.3703622817993164, 'learning_rate': 0.0002, 'epoch': 3.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.553, 'grad_norm': 1.3990672826766968, 'learning_rate': 0.0002, 'epoch': 3.34}\n",
      "{'loss': 1.505, 'grad_norm': 1.6944869756698608, 'learning_rate': 0.0002, 'epoch': 3.47}\n",
      "{'loss': 1.4556, 'grad_norm': 1.570709466934204, 'learning_rate': 0.0002, 'epoch': 3.6}\n",
      "{'loss': 1.448, 'grad_norm': 1.552973747253418, 'learning_rate': 0.0002, 'epoch': 3.73}\n",
      "{'loss': 1.6038, 'grad_norm': 1.9678901433944702, 'learning_rate': 0.0002, 'epoch': 3.86}\n",
      "{'loss': 1.5313, 'grad_norm': 2.1623756885528564, 'learning_rate': 0.0002, 'epoch': 3.99}\n",
      "{'loss': 1.3341, 'grad_norm': 1.736278772354126, 'learning_rate': 0.0002, 'epoch': 4.12}\n",
      "{'train_runtime': 382.893, 'train_samples_per_second': 3.343, 'train_steps_per_second': 0.836, 'train_loss': 1.8190350830554962, 'epoch': 4.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3263f94f3d444b0ab9e5c8a10dba6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/75.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a287f81623e43c2a33ccc54435ecb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba68e0de3865468094eb3e2872846afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Aditi25/InstructWithOurOwnData/commit/16dac3e4134e4fc84abf539fa18b31edae3992ae', commit_message='End of training', commit_description='', oid='16dac3e4134e4fc84abf539fa18b31edae3992ae', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#model.save_pretrained(\"output_dir\")\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9b733ddcd5401a9e2c78fda5e81a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/672 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BMSCE CSE\\.cache\\huggingface\\hub\\models--Aditi25--InstructWithOurOwnData. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3ad3212dce44c1af421d1ee87f1b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad88e657b54f477da38b446f2f7a9985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/75.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PEFT_MODEL = \"Aditi25/InstructWithOurOwnData\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "<human>: {question}\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "    # encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_start = \"<assistant>:\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BMSCE CSE\\Desktop\\Instruct\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Management switches are typically paired in a cluster to ensure proper communication and fault tolerance. The top switch usually serves as the primary switch and is responsible for distributing network traffic, while the bottom switch usually serves as a backup switch and is responsible for distributing power to the network equipment.\n",
      "User\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How are management switches typically paired in cluster systems, and what roles do the top and bottom switches in a stack usually play?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users should follow the procedure outlined in the cluster configuration guide to configure management switches with a cluster definition file. This involves creating a cluster definition file, specifying the switch model and cluster name, and then running the \"cm init-cluster\" command to create the cluster.\n",
      "User\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"What procedure should users follow to configure management switches with a cluster definition file?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def query_model(model, tokenizer, input_text):\n",
    "#     # Tokenize input text\n",
    "#     input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "#     # Generate response from the model\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(\n",
    "#             input_ids,\n",
    "#             do_sample=True,\n",
    "#             max_length=50,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             num_return_sequences=1,\n",
    "#         )\n",
    "    \n",
    "#     # Decode the generated response\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "#     return response\n",
    "\n",
    "# # Example usage\n",
    "# user_input = \"How are management switches typically paired in cluster systems, and what roles do the top and bottom switches in a stack usually play?\"\n",
    "# response = query_model(model, tokenizer, user_input)\n",
    "# print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
